{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.1\n",
    "\n",
    "$(Aw)_t = w_1^T\\tilde x(t)+w_0-y(t)w_2^T\\tilde x(t)$ ; $b_t=y(t)$\\\n",
    "Suppose $Aw = b$ then $b_t = y(t) = w_1^T\\tilde x(t)+w_0-y(t)w_2^T\\tilde x(t) \\Leftrightarrow y(t)(1+w_2^T\\tilde x(t)) = w_1^T\\tilde x(t)+w_0 \\Leftrightarrow y(t) = \\frac {w_1^T\\tilde x(t)+w_0}{1+w_2^T\\tilde x(t)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(722,)\n",
      "(361,)\n",
      "   Unnamed: 0                              TIME     VALUE BUILDING  \\\n",
      "0           0  2022-05-01 19:24:38.178000+00:00  348307.0    bat01   \n",
      "1           1  2022-05-02 16:04:38.148000+00:00  348497.0    bat01   \n",
      "2           2  2022-05-30 05:24:37.188000+00:00  353967.0    bat01   \n",
      "3           3  2022-05-03 06:32:38.127000+00:00      15.5    bat01   \n",
      "4           4  2022-05-04 11:28:38.085000+00:00       2.6    bat01   \n",
      "\n",
      "                   DETAILS                                      NAME  \\\n",
      "0  auxiliary_inverter_1_ge  bat01.r01.b.cfo.tgtbqb.dis124.cpt.ea.mes   \n",
      "1  auxiliary_inverter_1_ge  bat01.r01.b.cfo.tgtbqb.dis124.cpt.ea.mes   \n",
      "2  auxiliary_inverter_1_ge  bat01.r01.b.cfo.tgtbqb.dis124.cpt.ea.mes   \n",
      "3  auxiliary_inverter_1_ge  bat01.r01.b.cfo.tgtbqb.dis124.cpt.pa.mes   \n",
      "4  auxiliary_inverter_1_ge  bat01.r01.b.cfo.tgtbqb.dis124.cpt.pa.mes   \n",
      "\n",
      "                  SUBCATEGORY           LIBELLE UNITE  \n",
      "0  active_energy_auxiliary_ge    Energie active   kWh  \n",
      "1  active_energy_auxiliary_ge    Energie active   kWh  \n",
      "2  active_energy_auxiliary_ge    Energie active   kWh  \n",
      "3   active_power_auxiliary_ge  Puissance active    kW  \n",
      "4   active_power_auxiliary_ge  Puissance active    kW  \n"
     ]
    }
   ],
   "source": [
    "# Code from data_center_helper.py\n",
    "\n",
    "\n",
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('data_center_data_matrix.npy', allow_pickle=True)\n",
    "\n",
    "# Constructing matrices for min_w ||A w - b||_2**2\n",
    "\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std\n",
    "\n",
    "A = np.hstack([M, np.ones((M.shape[0],1)), -(M.T * COP_train[:,3]).T])\n",
    "b = COP_train[:,3]\n",
    "print(b.shape)\n",
    "\n",
    "# Constructing matrices for the test set\n",
    "\n",
    "M_test = (data_matrix_test - matrix_mean) / matrix_std\n",
    "A_test = np.hstack([M_test, np.ones((M_test.shape[0],1)), -(M_test.T * COP_test[:,3]).T])\n",
    "b_test = COP_test[:,3]\n",
    "print(b_test.shape)\n",
    "\n",
    "\n",
    "# Loading raw data\n",
    "data = pd.read_csv('Raw_Dataset_May.csv')\n",
    "print(data.head())\n",
    "\n",
    "def name_to_subcategory_and_details(col_name):\n",
    "    if np.isreal(col_name):\n",
    "        col_name = names[col_name]\n",
    "    indices = np.nonzero((data['NAME'] == col_name).values)[0]\n",
    "    if len(indices) > 0:\n",
    "        subcategory = data['SUBCATEGORY'].iloc[[indices[0]]].values[0]\n",
    "        details = data['DETAILS'].iloc[[indices[0]]].values[0]\n",
    "        return subcategory, details\n",
    "    else:\n",
    "        print('unknown name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maelm\\AppData\\Local\\Temp\\ipykernel_24240\\4101113602.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  x, res, rank, s  = np.linalg.lstsq(A,b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension de A: (722, 1785)\n",
      "The least square estimator is: [-0.00927821  0.08309371 -0.03672704 ...  0.01980595 -0.03057174\n",
      " -0.01188614]\n",
      "Sum of residuals: []\n",
      "Matrix A is of rank: 722\n"
     ]
    }
   ],
   "source": [
    "x, res, rank, s  = np.linalg.lstsq(A,b)\n",
    "print(\"Dimension de A:\", A.shape)\n",
    "print(\"The least square estimator is:\", x)\n",
    "print(\"Sum of residuals:\", res)\n",
    "print(\"Matrix A is of rank:\", rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least square difference of train set estimator: 140952.17552310223\n"
     ]
    }
   ],
   "source": [
    "diff = np.matmul(A_test,x)-b_test\n",
    "norm2 = np.linalg.norm(diff)**2\n",
    "print(\"Least square difference of train set estimator:\", 1/2*norm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the square of the norm of the difference is this large, maybe the training set is not able to provide a good solution with least square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.5\n",
    "$f_1 : w \\rightarrow \\frac{1}{2}||Aw-b||^2+\\frac{\\lambda}{2}||w||^2$\\\n",
    "$\\nabla f_1 = A^T(Aw-b)+\\lambda w = (A^TA+\\lambda I_n)w-A^Tb$, thus $\\nabla^2f_1 = A^TA+\\lambda I_n$\\\n",
    "Since $\\nabla^2f_1$ is definite positive $f_1$ is strictly convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maelm\\AppData\\Local\\Temp\\ipykernel_24240\\1631112457.py:2: RuntimeWarning: invalid value encountered in matmul\n",
      "  step = 1/np.linalg.norm(np.matmul(A.T,A)+lambd*np.identity(A.shape[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999735973418783\n",
      "Best estimator: [-0.01238055  0.05775866 -0.00111774 ...  0.01579912 -0.03571617\n",
      "  0.01335281]\n",
      "Number of iteration needed: 118766\n",
      "Regularized least square difference of train set estimator: 54737.17923169638\n"
     ]
    }
   ],
   "source": [
    "lambd = 100\n",
    "step = 1/np.linalg.norm(np.matmul(A.T,A)+lambd*np.identity(A.shape[1]))\n",
    "\n",
    "w = np.zeros(A.shape[1])\n",
    "gradient = np.matmul(A.T,np.matmul(A,w)-b)+lambd*w\n",
    "iter = 0\n",
    "while(np.linalg.norm(gradient)>1):\n",
    "    w = w - step*gradient\n",
    "    gradient = np.matmul(A.T,np.matmul(A,w)-b)+lambd*w\n",
    "    iter +=1\n",
    "\n",
    "print(np.linalg.norm(gradient))\n",
    "print(\"Best estimator:\",w)\n",
    "print(\"Number of iteration needed:\", iter)\n",
    "print(\"Regularized least square difference of train set estimator:\", 1/2*np.linalg.norm(np.matmul(A_test,w)-b_test)**2+lambd/2*np.linalg.norm(w)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization term: 224.30063273329952\n",
      "Proportion of the regularization term: 0.40977747827278416 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Regularization term:\",lambd/2*np.linalg.norm(w)**2)\n",
    "print(\"Proportion of the regularization term:\",100*lambd/2*np.linalg.norm(w)**2/(1/2*np.linalg.norm(np.matmul(A_test,w)-b_test)**2+lambd/2*np.linalg.norm(w)**2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.1\n",
    "$prox_{g_2}(x) = argmin_y g_2(y)+ \\frac {1}{2}||x-y||^2$\\\n",
    "$f_2(w)=\\frac {1}{2}||Aw-b||^2$ so $\\nabla f_2 = A^T(Aw-b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maelm\\AppData\\Local\\Temp\\ipykernel_24240\\218417930.py:2: RuntimeWarning: invalid value encountered in matmul\n",
      "  step = 1/np.linalg.norm(np.matmul(A.T,A))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.011110519438794314\n",
      "2\n",
      "0.007097333324298534\n",
      "3\n",
      "0.005097138677407172\n",
      "4\n",
      "0.0038546551023445346\n",
      "5\n",
      "0.0031141658426448674\n",
      "6\n",
      "0.00270523939591563\n",
      "7\n",
      "0.0024431554955535568\n",
      "8\n",
      "0.0022493722082329317\n",
      "9\n",
      "0.0021334668430295035\n",
      "10\n",
      "0.002026825880580864\n",
      "11\n",
      "0.0019453523897822508\n",
      "12\n",
      "0.0019024755836914832\n",
      "13\n",
      "0.0018587152728220596\n",
      "14\n",
      "0.0018192119759440158\n",
      "15\n",
      "0.0017787354884999877\n",
      "16\n",
      "0.0017516154182213352\n",
      "17\n",
      "0.0017191151056098405\n",
      "18\n",
      "0.001695235775257057\n",
      "19\n",
      "0.0016725018463676557\n",
      "20\n",
      "0.0016574392080668428\n",
      "21\n",
      "0.0016410452992265563\n",
      "22\n",
      "0.0016318572636036704\n",
      "23\n",
      "0.0016149414613021403\n",
      "24\n",
      "0.0016071046781523644\n",
      "25\n",
      "0.0015987238890863697\n",
      "26\n",
      "0.0015862138109430225\n",
      "27\n",
      "0.0015778486485501036\n",
      "28\n",
      "0.0015751926372849197\n",
      "29\n",
      "0.0015631120075121855\n",
      "30\n",
      "0.0015560731485806576\n",
      "31\n",
      "0.0015446237244254126\n",
      "32\n",
      "0.0015365296246529325\n",
      "33\n",
      "0.0015282150392949344\n",
      "34\n",
      "0.0015204700465105653\n",
      "35\n",
      "0.0015091309283447821\n",
      "36\n",
      "0.0014906366380010604\n",
      "37\n",
      "0.0014961003584801449\n",
      "38\n",
      "0.0014900588611331592\n",
      "39\n",
      "0.0014769996094254301\n",
      "40\n",
      "0.0014691914643377528\n",
      "41\n",
      "0.0014622687757763517\n",
      "42\n",
      "0.0014604533844150272\n",
      "43\n",
      "0.0014525384284479294\n",
      "44\n",
      "0.0014627424717492715\n",
      "45\n",
      "0.0014430100049235117\n",
      "46\n",
      "0.0014358127211189997\n",
      "47\n",
      "0.0014381808211511608\n",
      "48\n",
      "0.0014367172386856608\n",
      "49\n",
      "0.0014250293029927506\n",
      "50\n",
      "0.0014263734731023308\n",
      "51\n",
      "0.0014152432605887529\n",
      "52\n",
      "0.0014277784573828094\n",
      "53\n",
      "0.0014000810750220592\n",
      "54\n",
      "0.0014028286117867277\n",
      "55\n",
      "0.0014090176812099806\n",
      "56\n",
      "0.0013831899915009644\n",
      "57\n",
      "0.0013841149823205074\n",
      "58\n",
      "0.0013950252797816968\n",
      "59\n",
      "0.0013781789295820787\n",
      "60\n",
      "0.0013741836568510666\n",
      "61\n",
      "0.0013873016445010494\n",
      "62\n",
      "0.0013910109596191236\n",
      "63\n",
      "0.0013941872521275878\n",
      "64\n",
      "0.0013934676511276644\n",
      "65\n",
      "0.0013850541564531262\n",
      "66\n",
      "0.0013809662096264648\n",
      "67\n",
      "0.0013870406591520936\n",
      "68\n",
      "0.0013777883376328679\n",
      "69\n",
      "0.0013680246390889988\n",
      "70\n",
      "0.0013753258876677363\n",
      "71\n",
      "0.0013606462546327715\n",
      "72\n",
      "0.0013597186280784172\n",
      "73\n",
      "0.0013636547984194236\n",
      "74\n",
      "0.001352302314208179\n",
      "75\n",
      "0.0013557907290230827\n",
      "76\n",
      "0.0013542887686651154\n",
      "77\n",
      "0.0013442207849430597\n",
      "78\n",
      "0.0013400960920748\n",
      "79\n",
      "0.001351448006485526\n",
      "80\n",
      "0.0013392939045447935\n",
      "81\n",
      "0.0013303282704808248\n",
      "82\n",
      "0.0013348091361999224\n",
      "83\n",
      "0.0013327373670245115\n",
      "84\n",
      "0.0013243240940800764\n",
      "85\n",
      "0.0013351591465645726\n",
      "86\n",
      "0.0013215479533970787\n",
      "87\n",
      "0.0013256655211409095\n",
      "88\n",
      "0.00133220015979606\n",
      "89\n",
      "0.0013167472592737941\n",
      "90\n",
      "0.0013162225130043715\n",
      "91\n",
      "0.001325244056772049\n",
      "92\n",
      "0.001308382696834293\n",
      "93\n",
      "0.0013064932086467146\n",
      "94\n",
      "0.001312430044232419\n",
      "95\n",
      "0.0013043789583394472\n",
      "96\n",
      "0.0013016813455003437\n",
      "97\n",
      "0.001303769943824299\n",
      "98\n",
      "0.0013003916072507735\n",
      "99\n",
      "0.0013005683758115475\n",
      "100\n",
      "0.0013082012070958814\n",
      "101\n",
      "0.0012893022726411095\n",
      "102\n",
      "0.0012878938265395606\n",
      "103\n",
      "0.001301918466583293\n",
      "104\n",
      "0.0012906983264727519\n",
      "105\n",
      "0.0012858635377248182\n",
      "106\n",
      "0.001288282909752913\n",
      "107\n",
      "0.0012755763210337667\n",
      "108\n",
      "0.0012756861122942317\n",
      "109\n",
      "0.0012794221941628144\n",
      "110\n",
      "0.001237145432643584\n",
      "111\n",
      "0.0012354948504212857\n",
      "112\n",
      "0.0012356887739295014\n",
      "113\n",
      "0.001232112609608614\n",
      "114\n",
      "0.0012195908408293148\n",
      "115\n",
      "0.0012274158878338457\n",
      "116\n",
      "0.0012184953777145211\n",
      "117\n",
      "0.0012095990364604913\n",
      "118\n",
      "0.0012160473837315971\n",
      "119\n",
      "0.0012100839694926396\n",
      "120\n",
      "0.001206005848816782\n",
      "121\n",
      "0.0012100213301418207\n",
      "122\n",
      "0.0011943174706022066\n",
      "123\n",
      "0.001196636650347966\n",
      "124\n",
      "0.0011957912482254698\n",
      "125\n",
      "0.001182365424899769\n",
      "126\n",
      "0.0011854033999268237\n",
      "127\n",
      "0.0011958363588108994\n",
      "128\n",
      "0.001193637539572155\n",
      "129\n",
      "0.0011823592403078533\n",
      "130\n",
      "0.001179821496343486\n",
      "131\n",
      "0.001173484804433757\n",
      "132\n",
      "0.001180503276322807\n",
      "133\n",
      "0.0011761033493991866\n",
      "134\n",
      "0.0011674201076071588\n",
      "135\n",
      "0.0011746247225168356\n",
      "136\n",
      "0.0011688552937226435\n",
      "137\n",
      "0.0011644219357733957\n",
      "138\n",
      "0.0011718135478044917\n",
      "139\n",
      "0.0011698862032236156\n",
      "140\n",
      "0.0011589182490268158\n",
      "141\n",
      "0.0011604563770589908\n",
      "142\n",
      "0.0011597000119763555\n",
      "143\n",
      "0.001157701447611934\n",
      "144\n",
      "0.0011577933345511369\n",
      "145\n",
      "0.0011526719103535283\n",
      "146\n",
      "0.0011512491388136852\n",
      "147\n",
      "0.0011513307329288956\n",
      "148\n",
      "0.0011531868367255361\n",
      "149\n",
      "0.001148714220486347\n",
      "150\n",
      "0.0011455013653009808\n",
      "151\n",
      "0.001145534298971556\n",
      "152\n",
      "0.001146368369261713\n",
      "153\n",
      "0.0011391417730255268\n",
      "154\n",
      "0.0011407938353454328\n",
      "155\n",
      "0.0011341847884395228\n",
      "156\n",
      "0.0011405419592250797\n",
      "157\n",
      "0.0011323144614064837\n",
      "158\n",
      "0.001133741016178084\n",
      "159\n",
      "0.0011334486207854476\n",
      "160\n",
      "0.0011337285589241458\n",
      "161\n",
      "0.00112450111155831\n",
      "162\n",
      "0.0011268610901361458\n",
      "163\n",
      "0.001122585713319942\n",
      "164\n",
      "0.0011241156381055467\n",
      "165\n",
      "0.001119242106447611\n",
      "166\n",
      "0.0011172596898753138\n",
      "167\n",
      "0.0011172400245318948\n",
      "168\n",
      "0.0011175523133493946\n",
      "169\n",
      "0.001113207712923157\n",
      "170\n",
      "0.001109872763100414\n",
      "171\n",
      "0.001107663325576062\n",
      "172\n",
      "0.0011089311488091341\n",
      "173\n",
      "0.0011036586414787568\n",
      "174\n",
      "0.0011006025916863796\n",
      "175\n",
      "0.0011070984599851096\n",
      "176\n",
      "0.0011033538379056006\n",
      "177\n",
      "0.0011017523108357902\n",
      "178\n",
      "0.0011014703667098405\n",
      "179\n",
      "0.0010976671558722464\n",
      "180\n",
      "0.001098268673437223\n",
      "181\n",
      "0.001101532808711205\n",
      "182\n",
      "0.0010964202413075931\n",
      "183\n",
      "0.0010941818774860461\n",
      "184\n",
      "0.0010891614201796105\n",
      "185\n",
      "0.0010889061678921268\n",
      "186\n",
      "0.0010890952593192017\n",
      "187\n",
      "0.001092292588437315\n",
      "188\n",
      "0.0010804514530810526\n",
      "189\n",
      "0.0010860019107153502\n",
      "190\n",
      "0.001083189808938627\n",
      "191\n",
      "0.0010786123984951356\n",
      "192\n",
      "0.001081784284984274\n",
      "193\n",
      "0.0010897194245915762\n",
      "194\n",
      "0.0010770182924603811\n",
      "195\n",
      "0.0010750444011787918\n",
      "196\n",
      "0.0010760143310426706\n",
      "197\n",
      "0.0010791250961023702\n",
      "198\n",
      "0.0010738994268750279\n",
      "199\n",
      "0.0010758903702264338\n",
      "200\n",
      "0.001070388999866213\n",
      "201\n",
      "0.001074401605973884\n",
      "202\n",
      "0.001070714697524761\n",
      "203\n",
      "0.0010679866104085956\n",
      "204\n",
      "0.0010670071136569155\n",
      "205\n",
      "0.0010742298630474461\n",
      "206\n",
      "0.0010633074805176162\n",
      "207\n",
      "0.0010652748873332463\n",
      "208\n",
      "0.00106158834744114\n",
      "209\n",
      "0.0010689450277209843\n",
      "210\n",
      "0.0010613067542084363\n",
      "211\n",
      "0.001058365152443241\n",
      "212\n",
      "0.0010520859165902181\n",
      "213\n",
      "0.0010601422708285048\n",
      "214\n",
      "0.001062768917519591\n",
      "215\n",
      "0.0010568942076719017\n",
      "216\n",
      "0.0010520358205083188\n",
      "217\n",
      "0.001059424092808592\n",
      "218\n",
      "0.0010570948545674341\n",
      "219\n",
      "0.001053463747042372\n",
      "220\n",
      "0.0010474077228933673\n",
      "221\n",
      "0.0010619091727278316\n",
      "222\n",
      "0.00104859029873154\n",
      "223\n",
      "0.0010546848379547737\n",
      "224\n",
      "0.0010452789551740978\n",
      "225\n",
      "0.0010523490158891985\n",
      "226\n",
      "0.0010485989544910276\n",
      "227\n",
      "0.0010547220555230082\n",
      "228\n",
      "0.0010454738706018624\n",
      "229\n",
      "0.001051512418087019\n",
      "230\n",
      "0.0010466144939117405\n",
      "231\n",
      "0.0010469513136021308\n",
      "232\n",
      "0.001044928539384006\n",
      "233\n",
      "0.0010499182261409048\n",
      "234\n",
      "0.001042226373748238\n",
      "235\n",
      "0.0010453075511600407\n",
      "236\n",
      "0.001040504511782567\n",
      "237\n",
      "0.0010439060395860008\n",
      "238\n",
      "0.0010485949695396778\n",
      "239\n",
      "0.001051258147772581\n",
      "240\n",
      "0.00103669950222656\n",
      "241\n",
      "0.001038513617769286\n",
      "242\n",
      "0.0010353401063563149\n",
      "243\n",
      "0.001039951586940155\n",
      "244\n",
      "0.0010403481628335472\n",
      "245\n",
      "0.0010422581388027617\n",
      "246\n",
      "0.0010312231462811553\n",
      "247\n",
      "0.0010387288635045483\n",
      "248\n",
      "0.0010382493731704498\n",
      "249\n",
      "0.0010395139504816193\n",
      "250\n",
      "0.0010282596090587215\n",
      "251\n",
      "0.0010342586108306163\n",
      "252\n",
      "0.0010294014999218599\n",
      "253\n",
      "0.001031609958999355\n",
      "254\n",
      "0.0010323994760929169\n",
      "255\n",
      "0.0010333671246519082\n",
      "256\n",
      "0.0010261672807151461\n",
      "257\n",
      "0.0010313823270002348\n",
      "258\n",
      "0.0010259229323727767\n",
      "259\n",
      "0.0010271427996085111\n",
      "260\n",
      "0.0010240394628167758\n",
      "261\n",
      "0.001025390902665748\n",
      "262\n",
      "0.0010241251176975866\n",
      "263\n",
      "0.0010334449716578598\n",
      "264\n",
      "0.0010206137463377772\n",
      "265\n",
      "0.0010278979912271972\n",
      "266\n",
      "0.001020197491424796\n",
      "267\n",
      "0.0010286380356361428\n",
      "268\n",
      "0.001022872321153682\n",
      "269\n",
      "0.0010223619784215086\n",
      "270\n",
      "0.0010217337804211176\n",
      "271\n",
      "0.0010257385760237988\n",
      "272\n",
      "0.0010204684818577515\n",
      "273\n",
      "0.001025273665695316\n",
      "274\n",
      "0.0010215227551628643\n",
      "275\n",
      "0.0010243127939276037\n",
      "276\n",
      "0.0010178990067818573\n",
      "277\n",
      "0.0010249530989021327\n",
      "278\n",
      "0.001022247089890597\n",
      "279\n",
      "0.001023629084877281\n",
      "280\n",
      "0.0010150873218172489\n",
      "281\n",
      "0.0010197408461289916\n",
      "282\n",
      "0.001017451095535387\n",
      "283\n",
      "0.00102434238375528\n",
      "284\n",
      "0.0010183571892769126\n",
      "285\n",
      "0.0010180129081259833\n",
      "286\n",
      "0.0010210510039358495\n",
      "287\n",
      "0.001019932739603868\n",
      "288\n",
      "0.0010164315920355989\n",
      "289\n",
      "0.0010122085771904724\n",
      "290\n",
      "0.0010112901664581745\n",
      "291\n",
      "0.0010188013920328233\n",
      "292\n",
      "0.0010115961957992438\n",
      "293\n",
      "0.001011765662770686\n",
      "294\n",
      "0.0010175262420236133\n",
      "295\n",
      "0.001015427795255759\n",
      "296\n",
      "0.0010103710999729962\n",
      "297\n",
      "0.0010138029811216278\n",
      "298\n",
      "0.0010180163930669087\n",
      "299\n",
      "0.0010188779005730847\n",
      "300\n",
      "0.0010143375159990305\n",
      "301\n",
      "0.0010097429549524154\n",
      "302\n",
      "0.0010068493470873337\n",
      "303\n",
      "0.0010199698228542458\n",
      "304\n",
      "0.0010103409715425658\n",
      "305\n",
      "0.0010075196040424254\n",
      "306\n",
      "0.0010118092614950398\n",
      "307\n",
      "0.001010005773796859\n",
      "308\n",
      "0.001009243652337478\n",
      "309\n",
      "0.0010188557729031047\n",
      "310\n",
      "0.0010116671524215452\n",
      "311\n",
      "0.0010125899181781099\n",
      "312\n",
      "0.0010107695393598273\n",
      "313\n",
      "0.0010069458133944165\n",
      "314\n",
      "0.0010026671569073452\n",
      "315\n",
      "0.0010187263280701318\n",
      "316\n",
      "0.0010099423127902356\n",
      "317\n",
      "0.0010057872519792538\n",
      "318\n",
      "0.0010065587885124125\n",
      "319\n",
      "0.0009999729170850416\n",
      "Best estimator: [0.00000000e+00 7.51035714e-03 1.93276472e-03 ... 2.51638286e-05\n",
      " 0.00000000e+00 1.92262316e-05]\n",
      "Number of iteration needed: 319\n",
      "Regularized least square difference of train set estimator obtained from lasso problem: 2856.1419937396786\n"
     ]
    }
   ],
   "source": [
    "lambd = 200\n",
    "step = 1/np.linalg.norm(np.matmul(A.T,A))\n",
    "x = np.zeros(A.shape[1])\n",
    "gradient = np.matmul(A.T,np.matmul(A,x)-b)\n",
    "iter = 0\n",
    "\n",
    "\n",
    "def prox(y):\n",
    "    return np.maximum(np.sign(y)*(np.abs(y)-step*lambd),0)\n",
    "\n",
    "next = prox(x-step*gradient)\n",
    "while(np.linalg.norm(x-next)>1e-3):#if x = prox(x-gamma*gradf) then x = argmin f+g\n",
    "    x = next\n",
    "    gradient = np.matmul(A.T,np.matmul(A,x)-b)\n",
    "    iter += 1\n",
    "    next = prox(x-step*gradient)\n",
    "    print(iter)\n",
    "    print(np.linalg.norm(x-next))\n",
    "\n",
    "print(\"Best estimator:\",x)\n",
    "print(\"Number of iteration needed:\", iter)\n",
    "print(\"Regularized least square difference of train set estimator obtained from lasso problem:\",1/2*np.linalg.norm(np.matmul(A_test,x)-b_test)**2+lambd*np.linalg.norm(x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0009999729170850416\n",
      "Regularization term: 683.4944121677574\n",
      "Proportion of the regularization term: 23.9306873981019 %\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(x-next))\n",
    "print(\"Regularization term:\",lambd*np.linalg.norm(x,1))\n",
    "print(\"Proportion of the regularization term:\",100*lambd*np.linalg.norm(x,1)/(1/2*np.linalg.norm(np.matmul(A_test,x)-b_test)**2+lambd*np.linalg.norm(x,1)),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.0\n",
      "Best estimator: [0.00000000e+00 4.70357782e-05 4.70357782e-05 ... 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "Number of iteration needed: 1\n",
      "Regularized least square difference of train set estimator obtained from lasso problem: 13224.055048960341\n"
     ]
    }
   ],
   "source": [
    "lambd = 200\n",
    "b = 0.5\n",
    "t=1\n",
    "x = np.zeros(A.shape[1])\n",
    "gradient = np.matmul(A.T,np.matmul(A,x)-b)\n",
    "iter = 0\n",
    "\n",
    "\n",
    "def prox(y):\n",
    "    return np.maximum(np.sign(y)*(np.abs(y)-step*lambd),0)\n",
    "\n",
    "xplus = prox(x-t*gradient)\n",
    "while(1/2*np.linalg.norm(np.matmul(A,xplus)-b)**2> 1/2*np.linalg.norm(np.matmul(A,x)-b)**2+np.dot(gradient,xplus-x)+1/(2*t)*np.linalg.norm(xplus-x)**2):\n",
    "        t = b*t\n",
    "        xplus = prox(x-t*gradient)\n",
    "\n",
    "while(np.linalg.norm(x-xplus)>1e-3):#if x = prox(x-gamma*gradf) then x = argmin f+g\n",
    "    t=1\n",
    "    x = xplus\n",
    "    gradient = np.matmul(A.T,np.matmul(A,x)-b)\n",
    "    iter += 1\n",
    "    while(1/2*np.linalg.norm(np.matmul(A,xplus)-b)**2> 1/2*np.linalg.norm(np.matmul(A,x)-b)**2+np.dot(gradient,xplus-x)+1/(2*t)*np.linalg.norm(xplus-x)**2):\n",
    "        t = b*t\n",
    "        xplus = prox(x-t*gradient)\n",
    "    print(iter)\n",
    "    print(np.linalg.norm(x-xplus))\n",
    "\n",
    "print(\"Best estimator:\",x)\n",
    "print(\"Number of iteration needed:\", iter)\n",
    "print(\"Regularized least square difference of train set estimator obtained from lasso problem:\",1/2*np.linalg.norm(np.matmul(A_test,x)-b_test)**2+lambd*np.linalg.norm(x,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
